
Episode 0: total actions 170 episode reward 1.0
A.L.E: Arcade Learning Environment (version 0.7.5+db37282)
[Powered by Stella]
/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/gym/core.py:329: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.
  deprecation(
/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.
  deprecation(
/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. 
  logger.deprecation(
/Users/linasnasvytis/Desktop/Other/neuromatch/Project/nma2022/agent.py:80: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:204.)
  q_values = self.Q(torch.Tensor(state).unsqueeze(0))
Episode 1: total actions 130 episode reward 0.0
Episode 2: total actions 174 episode reward 1.0
Episode 3: total actions 130 episode reward 0.0
Episode 4: total actions 224 episode reward 2.0
Episode 5: total actions 204 episode reward 2.0
Traceback (most recent call last):
  File "/Users/linasnasvytis/Desktop/Other/neuromatch/Project/nma2022/agent.py", line 179, in <module>
    agent.train(50000)
  File "/Users/linasnasvytis/Desktop/Other/neuromatch/Project/nma2022/agent.py", line 80, in train
    q_values = self.Q(torch.Tensor(state).unsqueeze(0))
KeyboardInterrupt